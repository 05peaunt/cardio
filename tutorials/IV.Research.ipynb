{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep research of electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials you have learned about batches, pipelines and models. All those concepts are essential components of deep learning research, and CardIO provides convenient implementations of them for all users.\n",
    "But in order to perform real research, you need to conduct well-described, reproducible experiments and keep track on the results.\n",
    "\n",
    "For this task `research` module of `dataset` comes in handy. In this tutorial we will use `research` to train and test multiple variations of ResNet architecture in the task of atrial fibrillation classification.\n",
    "\n",
    "In this tutorial we will cover part of the functionality of `research`, but we strongly recommend to go through [Dataset's Research tutorial](https://github.com/analysiscenter/dataset/blob/master/examples/tutorials/08_research.ipynb) and to take a look at [documentation for Research](https://analysiscenter.github.io/dataset/intro/research.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are starting an experiment, the very first thing you should do is to write down the experimental design.  Clear and explicit design of experiment makes it easier to reproduces the results, because it helps to make shure that all important conditions are accounted.\n",
    "\n",
    "We will now follow this rule and step by step generate experimental desing, which later will be used as configuration for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing model variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall define which variations of ResNet architecture we are going to test.\n",
    "`ResNet` class from `dataset.models` has following configuration parameters:\n",
    "* number of filters after first convolution\n",
    "* layout in ResNet blocks\n",
    "* number of filters in ResNet blocks' convolutions\n",
    "* number of ResNet blocks between poolings\n",
    "\n",
    "We are going to tweak all of them! And, we suppose that you are already familiar with models from `dataset.models`.\n",
    "\n",
    "\n",
    "Along with custom architectures, let's try some classics, like `ResNet18` and `ResNet34`. To do this, we define \"model\" option of our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from cardio.dataset.models.tf import ResNet, ResNet18, ResNet34\n",
    "from cardio.dataset.research import Option, Grid, Research\n",
    "\n",
    "model_op1 = Option('model', [ResNet18, ResNet34])\n",
    "model_op2 = Option('model', [ResNet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have defined custom and classic ResNets in different options. Have some patience, in a few moments we will explain this move.\n",
    "\n",
    "Next, let's define options for input filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filters_op = Option('input_filters', [32, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now just follow the same procedure for the rest of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_op1 = Option('layout', ['cnacna'])\n",
    "layout_op2 = Option('layout', ['cna', 'cnacna'])\n",
    "\n",
    "blocks_op = Option('blocks', [[2, 3, 4, 5, 4, 3, 2], [2, 2, 2, 2, 2, 2, 2],\n",
    "                               [1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "filters_op = Option('filters', [[4, 8, 16, 32, 64, 128, 256], [4, 4, 8, 8, 16, 16, 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the time to explain why we define two options for each parameter.\n",
    "\n",
    "We have defined options for different parameters, but what are we going to do with them now? We will use them to generate a single grid of parameters, that will include all the combinations we want to test. \n",
    "\n",
    "Here, take a look at this simple example get the intuition about grid generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grid([[{'p1': ['1', '2']}, {'p2': ['v1', 'v2']}], [{'p1': ['4']}, {'p2': ['v3']}]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1 = Option('p1', [1, 2])\n",
    "op2 = Option('p2', ['v1', 'v2'])\n",
    "op3 = Option('p1', [4])\n",
    "op4 = Option('p2', ['v3'])\n",
    "\n",
    "grid = (op1 * op2 + op3 * op4)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform element-wise multipliction and add options. But, as far as `Grid` object is not very intuitive, we can generate all configurations from this grid and take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConfigAlias({'p1': '1', 'p2': 'v1'}),\n",
       " ConfigAlias({'p1': '1', 'p2': 'v2'}),\n",
       " ConfigAlias({'p1': '2', 'p2': 'v1'}),\n",
       " ConfigAlias({'p1': '2', 'p2': 'v2'}),\n",
       " ConfigAlias({'p1': '4', 'p2': 'v3'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grid.gen_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form is much simpler to understand! \n",
    "\n",
    "At this moment we are ready to generate the grid for our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = (model_op1 * layout_op1 + model_op2 * layout_op2 * input_filters_op * blocks_op * filters_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we're done with model variations. Now we shall move to the next part - the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just follow familiar procedure: setting up paths to the signals and the labels, them creating dataset instance and splitting data into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cardio import dataset as ds\n",
    "from cardio import EcgDataset\n",
    "\n",
    "PATH = '../cardio/tests/data/'#\"/notebooks/data/ECG/training2017\" # Change this path for your data dicrectory\n",
    "SIGNALS_MASK = os.path.join(PATH, \"A*.hea\")\n",
    "LABELS_PATH = os.path.join(PATH, \"REFERENCE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds = EcgDataset(path=SIGNALS_MASK, no_ext=True, sort=True)\n",
    "eds.split(0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here comes the interesting part! When we define `model_config`, we use `C('parameter_name')` for all the parameters we want to vary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cardio.dataset import F, B, C, V\n",
    "\n",
    "model_config = {\n",
    "    'inputs': dict(signals={'shape': F(lambda batch: batch.signal[0].shape[1:])},\n",
    "                   labels={'classes': ['A', 'NO'], 'transform': 'ohe', 'name': 'targets'}),\n",
    "    'input_block/inputs': 'signals',\n",
    "    \"loss\": \"ce\",\n",
    "    \"input_block/filters\": C('input_filters'),\n",
    "    \"body/block/layout\": C('layout'),\n",
    "    \"body/filters\": C('filters'),\n",
    "    \"body/num_blocks\": C('blocks'),\n",
    "    \"session/config\": tf.ConfigProto(allow_soft_placement=True),\n",
    "    \"device\": C(\"device\"),\n",
    "    \"optimizer\": \"Adam\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding values from configuration will be inserted instead of those templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is pretty short and simple - just setting up batch size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define root pipelines, wich contain only signal processing. Also, we will define a function to flip signals, whose R peaks turned down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "@njit(nogil=True)\n",
    "def center_flip(signal):\n",
    "    return np.random.choice(np.array([1, -1])) * (signal - np.mean(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train = (\n",
    "  ds.Pipeline()\n",
    "    .load(components=[\"signal\", \"meta\"], fmt=\"wfdb\")\n",
    "    .load(components=\"target\", fmt=\"csv\", src=LABELS_PATH)\n",
    "    .drop_labels([\"~\"])\n",
    "    .rename_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "    .apply_to_each_channel(center_flip)\n",
    "    .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "    .random_split_signals(3000, {\"A\": 6, \"NO\": 2})\n",
    "    .apply_transform(func=np.transpose, src='signal', dst='signal', axes=[0, 2, 1])\n",
    ").run(BATCH_SIZE, shuffle=True, drop_last=True, n_epochs=None, lazy=True)\n",
    "\n",
    "root_test = (\n",
    "  ds.Pipeline()\n",
    "    .load(components=[\"signal\", \"meta\"], fmt=\"wfdb\")\n",
    "    .load(components=\"target\", fmt=\"csv\", src=LABELS_PATH)\n",
    "    .drop_labels([\"~\"])\n",
    "    .rename_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "    .apply_to_each_channel(center_flip)\n",
    "    .split_signals(3000, 3000)\n",
    "    .apply_transform(func=np.transpose, src='signal', dst='signal', axes=[0, 2, 1])\n",
    ").run(BATCH_SIZE, shuffle=True, drop_last=True, n_epochs=1, lazy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define main pipelines. Training pipeline is pretty simlpe - we just train the model and save loss to pipeline variable.\n",
    "Again, we need to define simple function - this one prepares data from batch to be fed into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(batch, **kwagrs):\n",
    "    import numpy as np\n",
    "    n_reps = [signal.shape[0] for signal in batch.signal]\n",
    "    signals = np.array([segment for signal in batch.signal for segment in signal])\n",
    "    targets = np.repeat(batch.target, n_reps, axis=0)\n",
    "    return {\"feed_dict\": {'signals': signals, 'labels': targets}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = (\n",
    "  ds.Pipeline()\n",
    "    .init_variable('loss', init_on_each_run=list)\n",
    "    .init_model('dynamic', C('model'), 'model', config=model_config)\n",
    "    .train_model('model',\n",
    "                 make_data=make_data,\n",
    "                 fetches=[\"loss\"],\n",
    "                 save_to=[V(\"loss\")], mode=\"w\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pipeline is a bit more complex. At first, we initialize a variable to store metrics and save number of splits for each signal in batch component `splits`. Then, as usual, import model, make predictions and save predictions and targets into the batch components.\n",
    "\n",
    "As far as we make predictions for splits, we need to aggrageate them to obtain prediction for the whole signal. To do so, we will use `aggregate_crop_predictions` and update corresponding batch components.\n",
    "Then, pipeline method `gather_metrics` accumulates targets and predictions from batch and allows to calculate metrics afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_crop_predictions(arr, splits, agg_func, softmax=True, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if softmax:\n",
    "        arr -= np.max(arr, axis=1, keepdims=True)\n",
    "        arr_exp = np.exp(arr)\n",
    "        arr = (arr_exp / np.sum(arr_exp, axis=1, keepdims=True))\n",
    "    arr = np.split(arr, np.cumsum(splits)[:-1])\n",
    "    return np.array([agg_func(sig[:, 0]) for sig in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = (\n",
    "  ds.Pipeline()\n",
    "    .init_variable(\"metrics\", init_on_each_run=None)\n",
    "    .apply_transform(src=\"signal\", dst=\"splits\", func=lambda x: [x.shape[0]])\n",
    "    .import_model(\"model\", C(\"import_from\"))\n",
    "    .predict_model(\"model\", make_data=make_data,\n",
    "                   fetches=[\"predictions\", \"targets\"], \n",
    "                   save_to=[B('predictions'), B(\"targets\")])\n",
    "    .apply_transform_all(func=aggregate_crop_predictions,\n",
    "                         src='predictions', dst='predictions', \n",
    "                         splits=B('splits'), agg_func=np.mean)\n",
    "    .apply_transform_all(func=aggregate_crop_predictions, \n",
    "                         src='targets', dst='targets',\n",
    "                         splits=B('splits'), softmax=False, agg_func=np.max)\n",
    "    .gather_metrics('class', targets=B('targets'), predictions=B('predictions'),\n",
    "                    fmt='proba', save_to=V('metrics'), mode='u')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the experiment, we have to set how often to run test pipeline. Calculation of this value is not very strainghtforward because it should be stated in a number of iterations, not epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EACH_EPOCH = 20\n",
    "TRAIN_SIZE = len(eds.train)\n",
    "ITERATIONS = ((TRAIN_SIZE // BATCH_SIZE) + 1) * EPOCHS\n",
    "TEST_EXEC_FOR = ITERATIONS // EPOCHS * TEST_EACH_EPOCH\n",
    "STR_EXEC = '%{}'.format(TEST_EXEC_FOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will set a few constants for research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPS = 2\n",
    "N_BRANCHES = 1\n",
    "N_WORKERS = 1\n",
    "GPU = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of run of any pipeline we can execute some function. So let's write two functions to save number of trainable parameters and F1 score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_variables(iteration, experiment, ppl, model_name=\"model\"):\n",
    "    return experiment[ppl].pipeline.get_model_by_name(model_name).get_number_of_trainable_vars()\n",
    "\n",
    "\n",
    "def use_metrics(iteration, experiment, ppl, var_name):\n",
    "    metrics = experiment[ppl].pipeline.get_variable(var_name)\n",
    "    f1 = metrics.evaluate('f1_score', multiclass='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the whole research pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = (\n",
    "    Research()\n",
    "    .pipeline(root_train << eds, model_train,\n",
    "              variables=[\"loss\"], name=\"train\", dump=STR_EXEC)\n",
    "    .pipeline(root_test << eds, model_test,\n",
    "              name=\"test\", execute=STR_EXEC, dump=STR_EXEC,\n",
    "              import_from=\"train\", run=True)\n",
    "    .function(use_metrics, returns='metrics_f1', name='metrics_f1',\n",
    "              execute=STR_EXEC, dump=STR_EXEC, ppl='test', var_name='metrics')\n",
    "    .function(get_trainable_variables, returns='trainable_variables', \n",
    "              name='trainable_variables', execute=1, dump=1, ppl='train')\n",
    "    .grid(grid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research ResNet_research is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 52 jobs with 200 iterations. Totally: 10400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10400 [00:03<01:37, 102.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../cardio/dataset/dataset/models/tf/layers/pooling.py:150: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/master/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10400 [00:27<07:28, 21.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../cardio/dataset/dataset/models/tf/layers/pooling.py:150: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/master/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 608/10400 [00:46<12:21, 13.20it/s]Process Process-1:4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../cardio/dataset/dataset/research/distributor.py\", line 339, in _run_job\n",
      "    self.run_job()\n",
      "  File \"../cardio/dataset/dataset/research/workers.py\", line 69, in run_job\n",
      "    exceptions = job.parallel_call(iteration, unit_name, exec_actions)\n",
      "  File \"../cardio/dataset/dataset/decorators.py\", line 328, in wrapped_method\n",
      "    return wrap_with_threads(self, args, kwargs)\n",
      "  File \"../cardio/dataset/dataset/decorators.py\", line 225, in wrap_with_threads\n",
      "    one_ft = executor.submit(method, *margs, **mkwargs)\n",
      "  File \"/usr/lib/python3.6/concurrent/futures/thread.py\", line 123, in submit\n",
      "    self._adjust_thread_count()\n",
      "  File \"/usr/lib/python3.6/concurrent/futures/thread.py\", line 142, in _adjust_thread_count\n",
      "    t.start()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
      "    self._started.wait()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../cardio/dataset/dataset/research/distributor.py\", line 287, in __call__\n",
      "    signal = feedback_queue.get(timeout=1)\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/queues.py\", line 107, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/connection.py\", line 260, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/connection.py\", line 417, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/master/.local/lib/python3.6/site-packages/multiprocess/connection.py\", line 914, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-78c6f17a6e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mr.run(n_reps=N_REPS, n_iters=ITERATIONS, workers=N_WORKERS, #gpu=GPU, \n\u001b[0;32m----> 2\u001b[0;31m        branches=N_BRANCHES, name='ResNet_research', progress_bar=True)\n\u001b[0m",
      "\u001b[0;32m~/repos/ecg/cardio/dataset/dataset/research/research.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, n_reps, n_iters, workers, branches, name, progress_bar, gpu, worker_class, timeout, trails)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mdistr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistributor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         distr.run(self.jobs, dirname=self.name, n_jobs=self.n_jobs,\n\u001b[0;32m--> 308\u001b[0;31m                   n_iters=self.n_iters, progress_bar=self.progress_bar)\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/ecg/cardio/dataset/dataset/research/distributor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, jobs, dirname, n_jobs, n_iters, logfile, errorfile, progress_bar, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mr.run(n_reps=N_REPS, n_iters=ITERATIONS, workers=N_WORKERS, #gpu=GPU, \n",
    "       branches=N_BRANCHES, name='ResNet_research', progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
