{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and run pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 2 we learn pipelines. Pipeline is a sequence of actions we want to apply to batch. At first we describe what we want to do with batch. Then at some point in the code we pass dataset to pipeline and the caclucations actually run. Such \"lazy run\" makes code compact and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to define a pipeline. It simply lists actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"..\", \"..\", \"..\"))\n",
    "\n",
    "import ecg.dataset as ds\n",
    "from ecg.batch import EcgBatch\n",
    "\n",
    "preprocess_pipeline = (ds.Pipeline()\n",
    "                       .load(fmt=\"wfdb\", components=[\"signal\", \"meta\"])\n",
    "                       .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "                       .drop_short_signals(4000)\n",
    "                       .segment_signals(3000, 3000)\n",
    "                       .run(batch_size=300, shuffle=False, drop_last=False, n_epochs=1, lazy=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing to be clarified is the last action ```run```. We set parameter ```lazy=True``` since we want to run pipeline somewhere later. When we actually run this pipeline it will load batches of size ```batch_size``` and apply actions of pipeline. Iteration stops when no ecgs are left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an ecg dataset that we will pass to the pipeline (see [Notebook 1](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_1.ipynb) for details): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ds.FilesIndex(path=\"/notebooks/data/ECG/training2017/*.hea\", no_ext=True, sort=True)\n",
    "eds = ds.Dataset(index, batch_class=EcgBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start caclulation we pass ecg dataset into the pipeline and call an action ```run```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = (eds >> preprocess_pipeline).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that transformed ecgs are NOT assigned to ```processed```, which is a pipeline again. To save results you should add ```dump``` action to pipeline or save result into pipeline variable. How to work with pipeline variables will be explained in the [Notebook 3](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_3.ipynb) of tutorial. More features of pipelines are [here](https://github.com/analysiscenter/dataset/blob/master/doc/pipeline.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add custom action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to include in pipeline a new action called ```add_value``` that adds given value to each signal.\n",
    "New action requires two decorators:\n",
    "* ```@action``` enables using the action in pipeline\n",
    "* ```@inbatch_parallel``` splits batch into individual ecgs and process each ecg in a separate thread. \n",
    "\n",
    "Decorated function thus obtains index of ecg and every parameter passed from pipeline. From index we obtain position of corresponding signal in batch and add value to the signal. Combining it together be obtain the desired action:\n",
    "\n",
    "```python\n",
    "@ds.action\n",
    "@ds.inbatch_parallel(init=\"indices\", target=\"threads\")\n",
    "def add_value(self, index, value):\n",
    "    i = self.get_pos(None, \"signal\", index)\n",
    "    self.signal[i] += value\n",
    "```\n",
    "Action ```add_value``` now can be included in pipeline:\n",
    "```python\n",
    "ppl = (eds.pipeline()\n",
    "       .do_some_actions_before()\n",
    "       .add_value(0.01)\n",
    "       .do_some_actions_after())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next [Notebook 3](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_3.ipynb) we will work with models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
