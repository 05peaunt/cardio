{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 3 we learn how to train and predict built-in ECG models. We consider [fft_inceprion](https://github.com/analysiscenter/ecg/blob/unify_models/doc/fft_model.md) model as an example. This model learns to recognize atrial fibrillation (AF) from single lead ECG signal. Input of the model is ECG signal and meta, output is probability of signal being AF and non-AF. See more on ecg models [here](https://github.com/analysiscenter/ecg/blob/unify_models/doc/models.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary imports before to start. Note ```ModelEcgBatch``` that contains models is imported rather than plain ```EcgBatch```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"..\", \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import ecg.dataset as ds\n",
    "from ecg.batch import ModelEcgBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an ECG dataset (see [Notebook 1](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_1.ipynb) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = ds.FilesIndex(path=\"/notebooks/data/ECG/training2017/*.hea\", no_ext=True, sort=True)\n",
    "eds = ds.Dataset(index, batch_class=ModelEcgBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to divide the dataset into 2 parts that will be used for train and validation. Method ```cv_split``` do this job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eds.cv_split(0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 80% of the dataset are in ```eds.train``` and the rest in ```eds.test```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a preprocess pipeline. Here we\n",
    "* load signal, meta and target labels\n",
    "* drop noise signals\n",
    "* replace all non-AF labels with \"NO\" label\n",
    "* resample signal\n",
    "* drop too short signals\n",
    "* generate a number of segments from each signal\n",
    "* binarize labels to 0 and 1\n",
    "* prepare signal to expected model input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline = (ds.Pipeline()\n",
    "                       .load(fmt=\"wfdb\", components=[\"signal\", \"meta\"])\n",
    "                       .load(src=\"/notebooks/data/ECG/training2017/REFERENCE.csv\",\n",
    "                             fmt=\"csv\", components=\"target\")\n",
    "                       .drop_labels([\"~\"])\n",
    "                       .replace_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "                       .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "                       .drop_short_signals(4000)\n",
    "                       .segment_signals(3000, 3000)\n",
    "                       .binarize_labels()\n",
    "                       .apply(np.transpose, [0, 2, 1])\n",
    "                       .ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train pipeline is preprocess pipeline plus ```train_on_batch``` action. We exploit pipeline algebra to merge two pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ds.Pipeline() as p:\n",
    "    fft_train_pipeline = (preprocess_pipeline +\n",
    "                          p.train_on_batch('fft_inception', metrics=f1_score, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we only have to pass dataset to pipeline and start the calculation. Depending of your hardware training may take a while. Reduce ```n_epochs``` if you do not want to wait long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fft_trained = (eds.train >> fft_train_pipeline).run(batch_size=500, shuffle=True,\n",
    "                                                    drop_last=True, n_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we obtain ```fft_trained``` that contains trained model. Let's make a prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict pipeline is preprocess pipeline plus ```import_model``` action plus ```predict_on_batch``` action. Model can be imported from dump file or from pipeline in which model was trained. We show the second option since we have ```fft_trained```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_predict_pipeline = (ds.Pipeline()\n",
    "                        .import_model('fft_inception', fft_trained)\n",
    "                        .init_variable(\"prediction\", init=list, init_on_each_run=True)   \n",
    "                        .load(fmt=\"wfdb\", components=[\"signal\", \"meta\"])\n",
    "                        .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "                        .drop_short_signals(4000)\n",
    "                        .random_segment_signals(3000, n_segments=1)\n",
    "                        .apply(np.transpose, [0, 2, 1])\n",
    "                        .ravel()\n",
    "                        .predict_on_batch('fft_inception'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we aslo add action ```init_variable```. It defines empty list ```prediction``` that will store output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction on a sample ECG, say ECG with index \"A00001\". Create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ds.FilesIndex(path=\"/notebooks/data/ECG/training2017/A00001.hea\", no_ext=True)\n",
    "sample = ds.Dataset(index, batch_class=ModelEcgBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start caclulation we pass ```sample``` into pipeline and call action ```run```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = (sample >> fft_predict_pipeline).run(batch_size=1, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output we read pipeline variable ```prediction```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  4.89614147e-04,   9.99510407e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predicted.get_variable('prediction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is a list of probabilities for signal being AF and non-AF (the propabilities sum to 1). If the first value exceeds 0.5 we assign signal to be AF. In our example ```A00001``` is non-AF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Notebook 3. Here we learned:\n",
    "* how to train models\n",
    "* how to make predictions.\n",
    "\n",
    "See previous topics in [Notebook 1](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_1.ipynb) and [Notebook 2](https://github.com/analysiscenter/ecg/blob/unify_models/doc/ecg_tutorial_part_2.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
